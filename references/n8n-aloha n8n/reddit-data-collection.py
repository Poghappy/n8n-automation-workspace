#!/usr/bin/env python3
"""
Alohaæœ¬åœ°åŒåŸAIåº”ç”¨ - Redditæ•°æ®æ”¶é›†è„šæœ¬
ä½¿ç”¨Firecrawl APIæ”¶é›†æª€é¦™å±±ç›¸å…³Redditç¤¾åŒºæ•°æ®
"""

import requests
import json
import time
import csv
import os
from datetime import datetime
from typing import List, Dict, Any

class AlohaRedditCollector:
    def __init__(self, firecrawl_api_key: str):
        self.api_key = firecrawl_api_key
        self.base_url = "https://api.firecrawl.dev/v1"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # ç›®æ ‡Redditç¤¾åŒº
        self.target_subreddits = [
            "r/Hawaii",
            "r/Honolulu", 
            "r/HawaiiVisitors",
            "r/BigIsland"
        ]
        
        # å…³é”®è¯ç­›é€‰
        self.keywords = [
            "need", "looking for", "problem", "issue", "recommend", "help",
            "å›°éš¾", "éœ€è¦", "å¯»æ‰¾", "é—®é¢˜", "å»ºè®®", "æ¨è", "ç—›ç‚¹",
            "expensive", "cost", "price", "affordable", "cheap",
            "traffic", "parking", "transportation", "housing", "rent",
            "food", "restaurant", "grocery", "shopping",
            "healthcare", "doctor", "hospital", "clinic",
            "job", "work", "employment", "career",
            "tourist", "visitor", "local", "resident"
        ]
        
        self.collected_data = []
        
    def scrape_reddit_community(self, subreddit: str) -> Dict[str, Any]:
        """æŠ“å–Redditç¤¾åŒºæ•°æ®"""
        url = f"https://www.reddit.com/{subreddit}/"
        
        payload = {
            "url": url,
            "formats": ["markdown", "json"],
            "jsonOptions": {
                "prompt": f"""
                ä»{subreddit}ç¤¾åŒºä¸­æå–ç”¨æˆ·éœ€æ±‚å’Œç—›ç‚¹æ•°æ®ï¼Œé‡ç‚¹å…³æ³¨ï¼š
                1. ç”¨æˆ·é‡åˆ°çš„é—®é¢˜å’Œå›°éš¾
                2. å¯»æ±‚å¸®åŠ©æˆ–å»ºè®®çš„å¸–å­
                3. å¯¹æª€é¦™å±±/å¤å¨å¤·ç”Ÿæ´»çš„æŠ±æ€¨æˆ–å»ºè®®
                4. æœ¬åœ°æœåŠ¡éœ€æ±‚å’Œæ¨èè¯·æ±‚
                
                è¿”å›JSONæ ¼å¼ï¼š
                {{
                    "posts": [
                        {{
                            "title": "å¸–å­æ ‡é¢˜",
                            "content": "å¸–å­å†…å®¹æ‘˜è¦",
                            "author": "ä½œè€…",
                            "upvotes": "ç‚¹èµæ•°",
                            "comments_count": "è¯„è®ºæ•°",
                            "category": "éœ€æ±‚ç±»å‹",
                            "pain_points": ["ç—›ç‚¹1", "ç—›ç‚¹2"],
                            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
                            "urgency": "ç´§æ€¥ç¨‹åº¦(ä½/ä¸­/é«˜)",
                            "target_audience": "ç›®æ ‡ç”¨æˆ·ç¾¤ä½“"
                        }}
                    ]
                }}
                """
            },
            "onlyMainContent": True,
            "timeout": 60000
        }
        
        try:
            print(f"ğŸ” æ­£åœ¨æŠ“å– {subreddit} ç¤¾åŒºæ•°æ®...")
            response = requests.post(
                f"{self.base_url}/scrape",
                headers=self.headers,
                json=payload,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get("success"):
                    print(f"âœ… {subreddit} æ•°æ®æŠ“å–æˆåŠŸ")
                    return {
                        "subreddit": subreddit,
                        "data": result.get("data", {}),
                        "timestamp": datetime.now().isoformat(),
                        "status": "success"
                    }
                else:
                    print(f"âŒ {subreddit} æŠ“å–å¤±è´¥: {result.get('error', 'Unknown error')}")
                    return {"subreddit": subreddit, "status": "failed", "error": result.get('error')}
            else:
                print(f"âŒ HTTPé”™è¯¯ {response.status_code}: {response.text}")
                return {"subreddit": subreddit, "status": "http_error", "code": response.status_code}
                
        except Exception as e:
            print(f"âŒ æŠ“å– {subreddit} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            return {"subreddit": subreddit, "status": "exception", "error": str(e)}
    
    def search_hawaii_content(self, query: str) -> Dict[str, Any]:
        """æœç´¢å¤å¨å¤·ç›¸å…³å†…å®¹"""
        payload = {
            "query": f"{query} site:reddit.com (r/Hawaii OR r/Honolulu OR r/HawaiiVisitors OR r/BigIsland)",
            "limit": 10,
            "lang": "en",
            "country": "us",
            "scrapeOptions": {
                "formats": ["markdown", "json"],
                "jsonOptions": {
                    "prompt": f"""
                    åˆ†ææœç´¢ç»“æœä¸­å…³äº"{query}"çš„ç”¨æˆ·éœ€æ±‚å’Œç—›ç‚¹ï¼š
                    {{
                        "query": "{query}",
                        "findings": [
                            {{
                                "source": "æ¥æºURL",
                                "title": "æ ‡é¢˜",
                                "content": "ç›¸å…³å†…å®¹",
                                "pain_point": "è¯†åˆ«çš„ç—›ç‚¹",
                                "user_need": "ç”¨æˆ·éœ€æ±‚",
                                "solution_opportunity": "è§£å†³æ–¹æ¡ˆæœºä¼š"
                            }}
                        ]
                    }}
                    """
                },
                "onlyMainContent": True
            }
        }
        
        try:
            print(f"ğŸ” æœç´¢å…³é”®è¯: {query}")
            response = requests.post(
                f"{self.base_url}/search",
                headers=self.headers,
                json=payload,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get("success"):
                    print(f"âœ… æœç´¢ '{query}' æˆåŠŸ")
                    return {
                        "query": query,
                        "data": result.get("data", []),
                        "timestamp": datetime.now().isoformat(),
                        "status": "success"
                    }
            
            return {"query": query, "status": "failed"}
            
        except Exception as e:
            print(f"âŒ æœç´¢ '{query}' æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            return {"query": query, "status": "exception", "error": str(e)}
    
    def collect_all_data(self):
        """æ”¶é›†æ‰€æœ‰æ•°æ®"""
        print("ğŸï¸ å¼€å§‹æ”¶é›†Alohaæœ¬åœ°åŒåŸAIåº”ç”¨ç”¨æˆ·éœ€æ±‚æ•°æ®...")
        
        # 1. æŠ“å–å„ä¸ªRedditç¤¾åŒº
        for subreddit in self.target_subreddits:
            data = self.scrape_reddit_community(subreddit)
            self.collected_data.append(data)
            time.sleep(5)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        
        # 2. æœç´¢ç‰¹å®šå…³é”®è¯
        search_queries = [
            "Hawaii living problems",
            "Honolulu expensive cost",
            "Hawaii traffic parking issues",
            "Hawaii housing rent expensive",
            "Hawaii food grocery expensive",
            "Hawaii healthcare problems",
            "Hawaii job employment issues",
            "Hawaii tourist local problems"
        ]
        
        for query in search_queries:
            search_data = self.search_hawaii_content(query)
            self.collected_data.append(search_data)
            time.sleep(3)
        
        print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {len(self.collected_data)} ä¸ªæ•°æ®æº")
    
    def save_data(self, filename: str = None):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"aloha_reddit_data_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.collected_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        return filename
    
    def analyze_pain_points(self) -> Dict[str, Any]:
        """åˆ†ææ”¶é›†çš„ç—›ç‚¹æ•°æ®"""
        analysis = {
            "total_sources": len(self.collected_data),
            "successful_scrapes": len([d for d in self.collected_data if d.get("status") == "success"]),
            "pain_point_categories": {},
            "top_keywords": {},
            "urgency_distribution": {"é«˜": 0, "ä¸­": 0, "ä½": 0},
            "recommendations": []
        }
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„åˆ†æé€»è¾‘
        
        return analysis

def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    api_key = os.getenv("FIRECRAWL_API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½®FIRECRAWL_API_KEYç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºæ”¶é›†å™¨å®ä¾‹
    collector = AlohaRedditCollector(api_key)
    
    # æ”¶é›†æ•°æ®
    collector.collect_all_data()
    
    # ä¿å­˜æ•°æ®
    filename = collector.save_data()
    
    # åˆ†ææ•°æ®
    analysis = collector.analyze_pain_points()
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_filename = filename.replace('.json', '_analysis.json')
    with open(analysis_filename, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_filename}")
    print("\nğŸ‰ Alohaæœ¬åœ°åŒåŸAIåº”ç”¨æ•°æ®æ”¶é›†å®Œæˆï¼")

if __name__ == "__main__":
    main()
